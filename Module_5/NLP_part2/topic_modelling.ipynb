{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/bb/56f295a604dfafdef746cc81081ff4c6e825690de95963000300a1cd3d80/gensim-3.7.3-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (24.7MB)\n",
      "\u001b[K     |████████████████████████████████| 24.7MB 10.1MB/s eta 0:00:01    |█████████████                   | 10.0MB 5.2MB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /Users/flatironschool/anaconda3/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/flatironschool/anaconda3/lib/python3.7/site-packages (from gensim) (1.16.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/flatironschool/anaconda3/lib/python3.7/site-packages (from gensim) (1.2.1)\n",
      "Collecting smart-open>=1.7.0 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/c0/25d19badc495428dec6a4bf7782de617ee0246a9211af75b302a2681dea7/smart_open-1.8.4.tar.gz (63kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 3.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: boto>=2.32 in /Users/flatironschool/anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in /Users/flatironschool/anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (2.21.0)\n",
      "Collecting boto3 (from smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/43/6df1210d90ee286577ffc83f8715ea6c1c5ef11c9dc8031c6ff77936d735/boto3-1.9.183-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 11.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /Users/flatironschool/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Users/flatironschool/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/flatironschool/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/flatironschool/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.3.0,>=0.2.0 (from boto3->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl (70kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 10.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.13.0,>=1.12.183 (from boto3->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/25/650f208e32bad395c27061f8c25cc110ee6337aed64d7d690f7074ccdbb8/botocore-1.12.183-py2.py3-none-any.whl (5.6MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6MB 12.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /Users/flatironschool/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.183->boto3->smart-open>=1.7.0->gensim) (2.8.0)\n",
      "Requirement already satisfied: docutils>=0.10 in /Users/flatironschool/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.183->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/flatironschool/Library/Caches/pip/wheels/5f/ea/fb/5b1a947b369724063b2617011f1540c44eb00e28c3d2ca8692\n",
      "Successfully built smart-open\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.9.183 botocore-1.12.183 gensim-3.7.3 jmespath-0.9.4 s3transfer-0.2.1 smart-open-1.8.4\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/flatironschool/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.autos' 'comp.sys.mac.hardware' 'rec.motorcycles' 'misc.forsale'\n",
      " 'comp.os.ms-windows.misc' 'alt.atheism' 'comp.graphics'\n",
      " 'rec.sport.baseball' 'rec.sport.hockey' 'sci.electronics' 'sci.space'\n",
      " 'talk.politics.misc' 'sci.med' 'talk.politics.mideast'\n",
      " 'soc.religion.christian' 'comp.windows.x' 'comp.sys.ibm.pc.hardware'\n",
      " 'talk.politics.guns' 'talk.religion.misc' 'sci.crypt']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>From: irwin@cmptrc.lonestar.org (Irwin Arnstei...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>From: tchen@magnus.acs.ohio-state.edu (Tsung-K...</td>\n",
       "      <td>6</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  target  \\\n",
       "0     From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1     From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "10    From: irwin@cmptrc.lonestar.org (Irwin Arnstei...       8   \n",
       "100   From: tchen@magnus.acs.ohio-state.edu (Tsung-K...       6   \n",
       "1000  From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...       2   \n",
       "\n",
       "                 target_names  \n",
       "0                   rec.autos  \n",
       "1       comp.sys.mac.hardware  \n",
       "10            rec.motorcycles  \n",
       "100              misc.forsale  \n",
       "1000  comp.os.ms-windows.misc  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Import Dataset\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "print(df.target_names.unique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.machinelearningplus.com/wp-content/uploads/2018/03/Inferring-Topic-from-Keywords-1024x666.png' img/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    word = WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "#     print('token',word)\n",
    "    return stemmer.stem(word)\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a document to preview after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['From:', 'lerxst@wam.umd.edu', \"(where's\", 'my', 'thing)\\nSubject:', 'WHAT', 'car', 'is', 'this!?\\nNntp-Posting-Host:', 'rac3.wam.umd.edu\\nOrganization:', 'University', 'of', 'Maryland,', 'College', 'Park\\nLines:', '15\\n\\n', 'I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw\\nthe', 'other', 'day.', 'It', 'was', 'a', '2-door', 'sports', 'car,', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/\\nearly', '70s.', 'It', 'was', 'called', 'a', 'Bricklin.', 'The', 'doors', 'were', 'really', 'small.', 'In', 'addition,\\nthe', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body.', 'This', 'is', '\\nall', 'I', 'know.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name,', 'engine', 'specs,', 'years\\nof', 'production,', 'where', 'this', 'car', 'is', 'made,', 'history,', 'or', 'whatever', 'info', 'you\\nhave', 'on', 'this', 'funky', 'looking', 'car,', 'please', 'e-mail.\\n\\nThanks,\\n-', 'IL\\n', '', '', '----', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst', '----\\n\\n\\n\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['lerxst', 'thing', 'subject', 'nntp', 'post', 'host', 'organ', 'univers', 'maryland', 'colleg', 'park', 'line', 'wonder', 'enlighten', 'door', 'sport', 'look', 'late', 'earli', 'call', 'bricklin', 'door', 'small', 'addit', 'bumper', 'separ', 'rest', 'bodi', 'know', 'tellm', 'model', 'engin', 'spec', 'year', 'product', 'histori', 'info', 'funki', 'look', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = df.iloc[0].values[0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [lerxst, thing, subject, nntp, post, host, org...\n",
       "1        [guykuo, carson, washington, subject, clock, p...\n",
       "10       [irwin, cmptrc, lonestar, irwin, arnstein, sub...\n",
       "100      [tchen, magnus, ohio, state, tsung, chen, subj...\n",
       "1000     [dabl, lindbergh, subject, diamond, mous, curs...\n",
       "10000    [dseg, robert, loper, subject, nntp, post, hos...\n",
       "10001    [kimman, magnus, ohio, state, richard, subject...\n",
       "10002    [kwilson, casbah, acn, kirtley, wilson, subjec...\n",
       "10003    [subject, innoc, death, penalti, bobb, vice, r...\n",
       "10004    [livesey, solntz, livesey, subject, genocid, c...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = df['content'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words on the Data set\n",
    "Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 addit\n",
      "1 bodi\n",
      "2 bricklin\n",
      "3 bring\n",
      "4 bumper\n",
      "5 call\n",
      "6 colleg\n",
      "7 door\n",
      "8 earli\n",
      "9 engin\n",
      "10 enlighten\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out tokens that appear in:\n",
    "\n",
    "- less than 15 documents (absolute number) \n",
    "- more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
    "- after the above two steps, keep only the first 100000 most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim doc2bow\n",
    "\n",
    "For each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, 1),\n",
       " (20, 1),\n",
       " (26, 1),\n",
       " (30, 1),\n",
       " (138, 1),\n",
       " (142, 3),\n",
       " (244, 1),\n",
       " (271, 5),\n",
       " (403, 2),\n",
       " (593, 1),\n",
       " (598, 1),\n",
       " (630, 1),\n",
       " (657, 1),\n",
       " (701, 1),\n",
       " (815, 1),\n",
       " (836, 1),\n",
       " (877, 1),\n",
       " (921, 1),\n",
       " (985, 1),\n",
       " (1046, 1),\n",
       " (1388, 1),\n",
       " (1397, 1),\n",
       " (1568, 1),\n",
       " (1617, 1),\n",
       " (1685, 1),\n",
       " (1839, 2),\n",
       " (1897, 1),\n",
       " (2004, 1),\n",
       " (2502, 1),\n",
       " (2662, 1),\n",
       " (2709, 1),\n",
       " (2847, 1),\n",
       " (2957, 1),\n",
       " (3303, 1),\n",
       " (4583, 1),\n",
       " (4806, 1),\n",
       " (5079, 2),\n",
       " (5763, 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview Bag Of Words for our sample preprocessed document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 11 (\"host\") appears 1 time.\n",
      "Word 20 (\"nntp\") appears 1 time.\n",
      "Word 26 (\"spec\") appears 1 time.\n",
      "Word 30 (\"univers\") appears 1 time.\n",
      "Word 138 (\"state\") appears 1 time.\n",
      "Word 142 (\"window\") appears 3 time.\n",
      "Word 244 (\"richard\") appears 1 time.\n",
      "Word 271 (\"program\") appears 5 time.\n",
      "Word 403 (\"silver\") appears 2 time.\n",
      "Word 593 (\"secur\") appears 1 time.\n",
      "Word 598 (\"true\") appears 1 time.\n",
      "Word 630 (\"gate\") appears 1 time.\n",
      "Word 657 (\"econom\") appears 1 time.\n",
      "Word 701 (\"high\") appears 1 time.\n",
      "Word 815 (\"support\") appears 1 time.\n",
      "Word 836 (\"meet\") appears 1 time.\n",
      "Word 877 (\"correct\") appears 1 time.\n",
      "Word 921 (\"task\") appears 1 time.\n",
      "Word 985 (\"current\") appears 1 time.\n",
      "Word 1046 (\"major\") appears 1 time.\n",
      "Word 1388 (\"oper\") appears 1 time.\n",
      "Word 1397 (\"promis\") appears 1 time.\n",
      "Word 1568 (\"dept\") appears 1 time.\n",
      "Word 1617 (\"server\") appears 1 time.\n",
      "Word 1685 (\"user\") appears 1 time.\n",
      "Word 1839 (\"multi\") appears 2 time.\n",
      "Word 1897 (\"expect\") appears 1 time.\n",
      "Word 2004 (\"assur\") appears 1 time.\n",
      "Word 2502 (\"jose\") appears 1 time.\n",
      "Word 2662 (\"processor\") appears 1 time.\n",
      "Word 2709 (\"math\") appears 1 time.\n",
      "Word 2847 (\"rick\") appears 1 time.\n",
      "Word 2957 (\"overhead\") appears 1 time.\n",
      "Word 3303 (\"warner\") appears 1 time.\n",
      "Word 4583 (\"giant\") appears 1 time.\n",
      "Word 4806 (\"primarili\") appears 1 time.\n",
      "Word 5079 (\"billi\") appears 2 time.\n",
      "Word 5763 (\"app\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.16531831488632115),\n",
      " (1, 0.1678553823993299),\n",
      " (2, 0.15020052155842978),\n",
      " (3, 0.28581344134222897),\n",
      " (4, 0.11439130765694391),\n",
      " (5, 0.1516798683845623),\n",
      " (6, 0.3893283888350302),\n",
      " (7, 0.16890780449478127),\n",
      " (8, 0.12279484913135752),\n",
      " (9, 0.2634573656114004),\n",
      " (10, 0.16446021701525967),\n",
      " (11, 0.041907566184917734),\n",
      " (12, 0.13943742962367772),\n",
      " (13, 0.053228561863657146),\n",
      " (14, 0.17840678372959912),\n",
      " (15, 0.1614581045972935),\n",
      " (16, 0.10182359643822467),\n",
      " (17, 0.23339500537007382),\n",
      " (18, 0.1622034571062096),\n",
      " (19, 0.28046098400184816),\n",
      " (20, 0.04264750560541665),\n",
      " (21, 0.18494250912032378),\n",
      " (22, 0.14867573439400095),\n",
      " (23, 0.15971285457776704),\n",
      " (24, 0.18156677399111007),\n",
      " (25, 0.14477104789605966),\n",
      " (26, 0.20972103713602588),\n",
      " (27, 0.19290536120043997),\n",
      " (28, 0.08432238906696132),\n",
      " (29, 0.08377240739564006),\n",
      " (30, 0.04441833427243015),\n",
      " (31, 0.13721646110054028),\n",
      " (32, 0.08417832496773713)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using Bag of Words\n",
    "\n",
    "Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=20, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each topic, we will explore the words occuring in that topic and its relative weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.008*\"think\" + 0.007*\"know\" + 0.007*\"peopl\" + 0.006*\"wire\" + 0.006*\"articl\" + 0.006*\"like\" + 0.006*\"point\" + 0.005*\"problem\" + 0.004*\"question\" + 0.004*\"time\"\n",
      "Topic: 1 \n",
      "Words: 0.007*\"like\" + 0.006*\"columbia\" + 0.006*\"articl\" + 0.006*\"nntp\" + 0.006*\"host\" + 0.005*\"univers\" + 0.005*\"think\" + 0.004*\"team\" + 0.004*\"know\" + 0.004*\"peopl\"\n",
      "Topic: 2 \n",
      "Words: 0.006*\"server\" + 0.006*\"like\" + 0.005*\"think\" + 0.005*\"unit\" + 0.005*\"window\" + 0.004*\"state\" + 0.004*\"appear\" + 0.004*\"articl\" + 0.004*\"time\" + 0.004*\"file\"\n",
      "Topic: 3 \n",
      "Words: 0.007*\"articl\" + 0.007*\"say\" + 0.006*\"like\" + 0.006*\"good\" + 0.006*\"peopl\" + 0.005*\"know\" + 0.005*\"time\" + 0.005*\"think\" + 0.005*\"come\" + 0.004*\"need\"\n",
      "Topic: 4 \n",
      "Words: 0.008*\"articl\" + 0.008*\"like\" + 0.007*\"peopl\" + 0.007*\"univers\" + 0.007*\"think\" + 0.006*\"know\" + 0.005*\"year\" + 0.005*\"right\" + 0.004*\"time\" + 0.004*\"host\"\n",
      "Topic: 5 \n",
      "Words: 0.006*\"peopl\" + 0.006*\"like\" + 0.006*\"articl\" + 0.005*\"think\" + 0.005*\"year\" + 0.004*\"mail\" + 0.004*\"good\" + 0.004*\"host\" + 0.004*\"nntp\" + 0.004*\"internet\"\n",
      "Topic: 6 \n",
      "Words: 0.009*\"know\" + 0.008*\"pitt\" + 0.007*\"articl\" + 0.007*\"gordon\" + 0.006*\"like\" + 0.006*\"bank\" + 0.006*\"good\" + 0.005*\"time\" + 0.005*\"year\" + 0.005*\"think\"\n",
      "Topic: 7 \n",
      "Words: 0.017*\"drive\" + 0.009*\"disk\" + 0.008*\"articl\" + 0.008*\"state\" + 0.007*\"univers\" + 0.007*\"host\" + 0.007*\"nntp\" + 0.006*\"ohio\" + 0.006*\"control\" + 0.005*\"hard\"\n",
      "Topic: 8 \n",
      "Words: 0.008*\"host\" + 0.008*\"nntp\" + 0.008*\"like\" + 0.007*\"univers\" + 0.006*\"scsi\" + 0.006*\"know\" + 0.006*\"color\" + 0.005*\"good\" + 0.005*\"drive\" + 0.005*\"window\"\n",
      "Topic: 9 \n",
      "Words: 0.018*\"file\" + 0.015*\"window\" + 0.006*\"think\" + 0.005*\"know\" + 0.005*\"time\" + 0.005*\"host\" + 0.005*\"articl\" + 0.005*\"peopl\" + 0.005*\"list\" + 0.004*\"mail\"\n",
      "Topic: 10 \n",
      "Words: 0.009*\"articl\" + 0.006*\"host\" + 0.006*\"nntp\" + 0.006*\"like\" + 0.006*\"peopl\" + 0.005*\"know\" + 0.004*\"work\" + 0.004*\"think\" + 0.004*\"right\" + 0.004*\"univers\"\n",
      "Topic: 11 \n",
      "Words: 0.011*\"articl\" + 0.010*\"univers\" + 0.008*\"know\" + 0.007*\"host\" + 0.007*\"nntp\" + 0.006*\"world\" + 0.005*\"distribut\" + 0.005*\"window\" + 0.005*\"mail\" + 0.005*\"repli\"\n",
      "Topic: 12 \n",
      "Words: 0.010*\"peopl\" + 0.008*\"think\" + 0.007*\"articl\" + 0.006*\"come\" + 0.006*\"time\" + 0.005*\"univers\" + 0.005*\"say\" + 0.005*\"like\" + 0.005*\"moral\" + 0.004*\"believ\"\n",
      "Topic: 13 \n",
      "Words: 0.010*\"peopl\" + 0.009*\"know\" + 0.008*\"say\" + 0.007*\"articl\" + 0.007*\"go\" + 0.007*\"think\" + 0.006*\"like\" + 0.006*\"time\" + 0.006*\"right\" + 0.005*\"state\"\n",
      "Topic: 14 \n",
      "Words: 0.010*\"program\" + 0.007*\"work\" + 0.007*\"window\" + 0.006*\"like\" + 0.006*\"card\" + 0.006*\"need\" + 0.006*\"imag\" + 0.006*\"univers\" + 0.006*\"time\" + 0.006*\"articl\"\n",
      "Topic: 15 \n",
      "Words: 0.017*\"game\" + 0.012*\"team\" + 0.009*\"play\" + 0.008*\"year\" + 0.008*\"univers\" + 0.007*\"hockey\" + 0.006*\"host\" + 0.006*\"season\" + 0.006*\"nntp\" + 0.005*\"player\"\n",
      "Topic: 16 \n",
      "Words: 0.018*\"space\" + 0.013*\"nasa\" + 0.008*\"articl\" + 0.006*\"host\" + 0.006*\"nntp\" + 0.006*\"univers\" + 0.005*\"file\" + 0.005*\"drive\" + 0.005*\"time\" + 0.005*\"work\"\n",
      "Topic: 17 \n",
      "Words: 0.011*\"jesus\" + 0.011*\"christian\" + 0.008*\"think\" + 0.008*\"say\" + 0.007*\"know\" + 0.007*\"believ\" + 0.006*\"univers\" + 0.005*\"peopl\" + 0.005*\"exist\" + 0.004*\"thing\"\n",
      "Topic: 18 \n",
      "Words: 0.022*\"armenian\" + 0.012*\"turkish\" + 0.008*\"peopl\" + 0.007*\"articl\" + 0.007*\"turk\" + 0.006*\"right\" + 0.006*\"armenia\" + 0.005*\"world\" + 0.005*\"argic\" + 0.005*\"like\"\n",
      "Topic: 19 \n",
      "Words: 0.010*\"encrypt\" + 0.009*\"chip\" + 0.007*\"clipper\" + 0.006*\"articl\" + 0.006*\"secur\" + 0.006*\"govern\" + 0.005*\"know\" + 0.005*\"peopl\" + 0.004*\"year\" + 0.004*\"think\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.004*\"window\" + 0.002*\"file\" + 0.002*\"dyer\" + 0.002*\"alaska\" + 0.002*\"sale\" + 0.002*\"program\" + 0.002*\"card\" + 0.002*\"know\" + 0.002*\"problem\" + 0.002*\"like\"\n",
      "Topic: 1 Word: 0.003*\"buffalo\" + 0.002*\"virginia\" + 0.002*\"jesus\" + 0.002*\"peopl\" + 0.002*\"bontchev\" + 0.002*\"univers\" + 0.002*\"hamburg\" + 0.002*\"cramer\" + 0.002*\"articl\" + 0.002*\"brian\"\n",
      "Topic: 2 Word: 0.002*\"psuvm\" + 0.002*\"drive\" + 0.002*\"window\" + 0.002*\"univers\" + 0.002*\"know\" + 0.002*\"card\" + 0.002*\"caltech\" + 0.002*\"think\" + 0.002*\"game\" + 0.002*\"iastat\"\n",
      "Topic: 3 Word: 0.003*\"card\" + 0.003*\"scsi\" + 0.003*\"wire\" + 0.002*\"driver\" + 0.002*\"access\" + 0.002*\"diamond\" + 0.002*\"univers\" + 0.002*\"hook\" + 0.002*\"christian\" + 0.002*\"video\"\n",
      "Topic: 4 Word: 0.004*\"sandvik\" + 0.003*\"drive\" + 0.002*\"card\" + 0.002*\"netcom\" + 0.002*\"david\" + 0.002*\"appl\" + 0.002*\"clipper\" + 0.002*\"kent\" + 0.002*\"softwar\" + 0.002*\"inform\"\n",
      "Topic: 5 Word: 0.003*\"hulman\" + 0.002*\"window\" + 0.002*\"file\" + 0.002*\"simm\" + 0.002*\"univers\" + 0.002*\"widget\" + 0.002*\"rise\" + 0.002*\"program\" + 0.002*\"host\" + 0.002*\"nntp\"\n",
      "Topic: 6 Word: 0.005*\"gordon\" + 0.004*\"pitt\" + 0.004*\"bank\" + 0.002*\"articl\" + 0.002*\"cadr\" + 0.002*\"think\" + 0.002*\"right\" + 0.002*\"surrend\" + 0.002*\"peopl\" + 0.002*\"chastiti\"\n",
      "Topic: 7 Word: 0.003*\"game\" + 0.002*\"keith\" + 0.002*\"muenchen\" + 0.002*\"drive\" + 0.002*\"okcforum\" + 0.002*\"columbia\" + 0.002*\"like\" + 0.002*\"univers\" + 0.002*\"know\" + 0.002*\"caltech\"\n",
      "Topic: 8 Word: 0.003*\"window\" + 0.003*\"chip\" + 0.003*\"intercon\" + 0.003*\"duke\" + 0.002*\"amanda\" + 0.002*\"uiuc\" + 0.002*\"encrypt\" + 0.002*\"austin\" + 0.002*\"clipper\" + 0.002*\"shearson\"\n",
      "Topic: 9 Word: 0.004*\"disk\" + 0.003*\"armenian\" + 0.003*\"drive\" + 0.003*\"serdar\" + 0.003*\"argic\" + 0.002*\"floppi\" + 0.002*\"file\" + 0.002*\"univers\" + 0.002*\"window\" + 0.002*\"know\"\n",
      "Topic: 10 Word: 0.004*\"window\" + 0.003*\"bike\" + 0.003*\"columbia\" + 0.003*\"chip\" + 0.002*\"sdsu\" + 0.002*\"larc\" + 0.002*\"cunixb\" + 0.002*\"clipper\" + 0.002*\"escrow\" + 0.002*\"univers\"\n",
      "Topic: 11 Word: 0.003*\"window\" + 0.003*\"seizur\" + 0.003*\"tammi\" + 0.002*\"walla\" + 0.002*\"healta\" + 0.002*\"david\" + 0.002*\"duke\" + 0.002*\"cray\" + 0.002*\"univers\" + 0.002*\"look\"\n",
      "Topic: 12 Word: 0.003*\"peopl\" + 0.002*\"christian\" + 0.002*\"window\" + 0.002*\"know\" + 0.002*\"think\" + 0.002*\"homosexu\" + 0.002*\"world\" + 0.002*\"like\" + 0.002*\"polygon\" + 0.002*\"univers\"\n",
      "Topic: 13 Word: 0.004*\"israel\" + 0.003*\"isra\" + 0.003*\"arab\" + 0.002*\"peopl\" + 0.002*\"game\" + 0.002*\"right\" + 0.002*\"team\" + 0.002*\"think\" + 0.002*\"state\" + 0.002*\"year\"\n",
      "Topic: 14 Word: 0.004*\"window\" + 0.003*\"nasa\" + 0.003*\"toronto\" + 0.002*\"henri\" + 0.002*\"file\" + 0.002*\"program\" + 0.002*\"know\" + 0.002*\"thank\" + 0.002*\"color\" + 0.002*\"univers\"\n",
      "Topic: 15 Word: 0.004*\"jesus\" + 0.004*\"stratus\" + 0.003*\"christian\" + 0.002*\"believ\" + 0.002*\"peopl\" + 0.002*\"say\" + 0.002*\"church\" + 0.002*\"think\" + 0.002*\"bibl\" + 0.002*\"uchicago\"\n",
      "Topic: 16 Word: 0.004*\"upenn\" + 0.003*\"mail\" + 0.003*\"printer\" + 0.003*\"henri\" + 0.003*\"tempest\" + 0.002*\"msstate\" + 0.002*\"rochest\" + 0.002*\"univers\" + 0.002*\"window\" + 0.002*\"thank\"\n",
      "Topic: 17 Word: 0.004*\"encrypt\" + 0.003*\"govern\" + 0.003*\"chip\" + 0.002*\"captain\" + 0.002*\"like\" + 0.002*\"clipper\" + 0.002*\"stanford\" + 0.002*\"peopl\" + 0.002*\"univers\" + 0.002*\"know\"\n",
      "Topic: 18 Word: 0.002*\"year\" + 0.002*\"know\" + 0.002*\"team\" + 0.002*\"drive\" + 0.002*\"window\" + 0.002*\"game\" + 0.002*\"time\" + 0.002*\"space\" + 0.002*\"like\" + 0.002*\"mous\"\n",
      "Topic: 19 Word: 0.002*\"host\" + 0.002*\"uiuc\" + 0.002*\"nntp\" + 0.002*\"file\" + 0.002*\"know\" + 0.002*\"univers\" + 0.002*\"articl\" + 0.002*\"say\" + 0.002*\"christian\" + 0.002*\"cleveland\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=20, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model\n",
    "We will check where our test document would be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['graham',\n",
       " 'toal',\n",
       " 'gtoal',\n",
       " 'gtoal',\n",
       " 'subject',\n",
       " 'hard',\n",
       " 'drive',\n",
       " 'secur',\n",
       " 'target',\n",
       " 'origin',\n",
       " 'gtoal',\n",
       " 'pizzabox',\n",
       " 'demon',\n",
       " 'keyword',\n",
       " 'entropi',\n",
       " 'nntp',\n",
       " 'post',\n",
       " 'host',\n",
       " 'pizzabox',\n",
       " 'demon',\n",
       " 'repli',\n",
       " 'graham',\n",
       " 'toal',\n",
       " 'gtoal',\n",
       " 'gtoal',\n",
       " 'organ',\n",
       " 'cuddlehog',\n",
       " 'anonym',\n",
       " 'line',\n",
       " 'articl',\n",
       " 'kean',\n",
       " 'write',\n",
       " 'matter',\n",
       " 'fact',\n",
       " 'random',\n",
       " 'file',\n",
       " 'disk',\n",
       " 'reason',\n",
       " 'special',\n",
       " 'purpos',\n",
       " 'hardwar',\n",
       " 'take',\n",
       " 'long',\n",
       " 'time',\n",
       " 'generat',\n",
       " 'good',\n",
       " 'random',\n",
       " 'bit',\n",
       " 'program',\n",
       " 'crank',\n",
       " 'coupl',\n",
       " 'bit',\n",
       " 'minut',\n",
       " 'pretti',\n",
       " 'conserv',\n",
       " 'time',\n",
       " 'need',\n",
       " 'sound',\n",
       " 'like',\n",
       " 'use',\n",
       " 'program',\n",
       " 'interest',\n",
       " 'post',\n",
       " 'sourc']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.7221387624740601\t \n",
      "Topic: 0.010*\"program\" + 0.007*\"work\" + 0.007*\"window\" + 0.006*\"like\" + 0.006*\"card\" + 0.006*\"need\" + 0.006*\"imag\" + 0.006*\"univers\" + 0.006*\"time\" + 0.006*\"articl\"\n",
      "\n",
      "Score: 0.17414069175720215\t \n",
      "Topic: 0.018*\"file\" + 0.015*\"window\" + 0.006*\"think\" + 0.005*\"know\" + 0.005*\"time\" + 0.005*\"host\" + 0.005*\"articl\" + 0.005*\"peopl\" + 0.005*\"list\" + 0.004*\"mail\"\n",
      "\n",
      "Score: 0.08600194752216339\t \n",
      "Topic: 0.017*\"game\" + 0.012*\"team\" + 0.009*\"play\" + 0.008*\"year\" + 0.008*\"univers\" + 0.007*\"hockey\" + 0.006*\"host\" + 0.006*\"season\" + 0.006*\"nntp\" + 0.005*\"player\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.7045295834541321\t \n",
      "Topic: 0.003*\"peopl\" + 0.002*\"christian\" + 0.002*\"window\" + 0.002*\"know\" + 0.002*\"think\" + 0.002*\"homosexu\" + 0.002*\"world\" + 0.002*\"like\" + 0.002*\"polygon\" + 0.002*\"univers\"\n",
      "\n",
      "Score: 0.2691435217857361\t \n",
      "Topic: 0.004*\"window\" + 0.002*\"file\" + 0.002*\"dyer\" + 0.002*\"alaska\" + 0.002*\"sale\" + 0.002*\"program\" + 0.002*\"card\" + 0.002*\"know\" + 0.002*\"problem\" + 0.002*\"like\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model on unseen document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.762354850769043\t Topic: 0.006*\"server\" + 0.006*\"like\" + 0.005*\"think\" + 0.005*\"unit\" + 0.005*\"window\"\n",
      "Score: 0.012507638894021511\t Topic: 0.018*\"file\" + 0.015*\"window\" + 0.006*\"think\" + 0.005*\"know\" + 0.005*\"time\"\n",
      "Score: 0.012507637962698936\t Topic: 0.008*\"think\" + 0.007*\"know\" + 0.007*\"peopl\" + 0.006*\"wire\" + 0.006*\"articl\"\n",
      "Score: 0.012507637962698936\t Topic: 0.007*\"like\" + 0.006*\"columbia\" + 0.006*\"articl\" + 0.006*\"nntp\" + 0.006*\"host\"\n",
      "Score: 0.012507637962698936\t Topic: 0.007*\"articl\" + 0.007*\"say\" + 0.006*\"like\" + 0.006*\"good\" + 0.006*\"peopl\"\n",
      "Score: 0.012507637962698936\t Topic: 0.008*\"articl\" + 0.008*\"like\" + 0.007*\"peopl\" + 0.007*\"univers\" + 0.007*\"think\"\n",
      "Score: 0.012507637962698936\t Topic: 0.006*\"peopl\" + 0.006*\"like\" + 0.006*\"articl\" + 0.005*\"think\" + 0.005*\"year\"\n",
      "Score: 0.012507637962698936\t Topic: 0.009*\"know\" + 0.008*\"pitt\" + 0.007*\"articl\" + 0.007*\"gordon\" + 0.006*\"like\"\n",
      "Score: 0.012507637962698936\t Topic: 0.017*\"drive\" + 0.009*\"disk\" + 0.008*\"articl\" + 0.008*\"state\" + 0.007*\"univers\"\n",
      "Score: 0.012507637962698936\t Topic: 0.008*\"host\" + 0.008*\"nntp\" + 0.008*\"like\" + 0.007*\"univers\" + 0.006*\"scsi\"\n",
      "Score: 0.012507637962698936\t Topic: 0.009*\"articl\" + 0.006*\"host\" + 0.006*\"nntp\" + 0.006*\"like\" + 0.006*\"peopl\"\n",
      "Score: 0.012507637962698936\t Topic: 0.011*\"articl\" + 0.010*\"univers\" + 0.008*\"know\" + 0.007*\"host\" + 0.007*\"nntp\"\n",
      "Score: 0.012507637962698936\t Topic: 0.010*\"peopl\" + 0.008*\"think\" + 0.007*\"articl\" + 0.006*\"come\" + 0.006*\"time\"\n",
      "Score: 0.012507637962698936\t Topic: 0.010*\"peopl\" + 0.009*\"know\" + 0.008*\"say\" + 0.007*\"articl\" + 0.007*\"go\"\n",
      "Score: 0.012507637962698936\t Topic: 0.010*\"program\" + 0.007*\"work\" + 0.007*\"window\" + 0.006*\"like\" + 0.006*\"card\"\n",
      "Score: 0.012507637962698936\t Topic: 0.017*\"game\" + 0.012*\"team\" + 0.009*\"play\" + 0.008*\"year\" + 0.008*\"univers\"\n",
      "Score: 0.012507637962698936\t Topic: 0.018*\"space\" + 0.013*\"nasa\" + 0.008*\"articl\" + 0.006*\"host\" + 0.006*\"nntp\"\n",
      "Score: 0.012507637962698936\t Topic: 0.011*\"jesus\" + 0.011*\"christian\" + 0.008*\"think\" + 0.008*\"say\" + 0.007*\"know\"\n",
      "Score: 0.012507637962698936\t Topic: 0.022*\"armenian\" + 0.012*\"turkish\" + 0.008*\"peopl\" + 0.007*\"articl\" + 0.007*\"turk\"\n",
      "Score: 0.012507637962698936\t Topic: 0.010*\"encrypt\" + 0.009*\"chip\" + 0.007*\"clipper\" + 0.006*\"articl\" + 0.006*\"secur\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Model Perplexity and Coherence Score\n",
    "Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is.  Topic coherence score, in particular, has been more helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.76994647193953\n",
      "\n",
      "Coherence Score:  0.43388944506097127\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(bow_corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "    https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
