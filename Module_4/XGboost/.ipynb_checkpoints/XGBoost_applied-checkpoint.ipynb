{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an XGboosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the appropriate packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n",
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost's hyperparameters\n",
    "\n",
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them [here](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters). But the most common ones that you should know are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall parameters have been divided into 3 categories by XGBoost authors:\n",
    "\n",
    "- **General Parameters:** Guide the overall functioning\n",
    "- **Booster Parameters:** Guide the individual booster (tree/regression) at each step\n",
    "- **Learning Task Parameters:** Guide the optimization performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Parameters\n",
    "These define the overall functionality of XGBoost.\n",
    "\n",
    "- **booster** [default=gbtree]\n",
    "Select the type of model to run at each iteration. It has 2 options:\n",
    "    - gbtree: tree-based models\n",
    "    - gblinear: linear models\n",
    "    \n",
    "- **silent** [default=0]:\n",
    "Silent mode is activated is set to 1, i.e. no running messages will be printed. It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "\n",
    "- **nthread**  [default to maximum number of threads available if not set]\n",
    "This is used for parallel processing and number of cores in the system should be entered. If you wish to run on all cores, value should not be entered and algorithm will detect automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "Though there are 2 types of boosters, we’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.\n",
    "\n",
    "- **eta [default=0.3]**\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "- **min_child_weight [default=1]**\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "- **max_depth [default=6]**\n",
    "    - The maximum depth of a tree, same as GBM.\n",
    "    - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Should be tuned using CV.\n",
    "    - Typical values: 3-10\n",
    "- **max_leaf_nodes**\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "- **gamma [default=0]**\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "- **max_delta_step [default=0]**\n",
    "    - In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "    - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "    - This is generally not used but you can explore further if you wish.\n",
    "- **subsample [default=1]**\n",
    "    - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bytree [default=1]**\n",
    "    - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bylevel [default=1]**\n",
    "    - Denotes the subsample ratio of columns for each split, in each level.\n",
    "    - I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "- **lambda [default=1]**\n",
    "    - L2 regularization term on weights (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "- **alpha [default=0]**\n",
    "    - L1 regularization term on weight (analogous to Lasso regression)\n",
    "    - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "- **scale_pos_weight [default=1]**\n",
    "    - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Task Parameters\n",
    "\n",
    "These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "\n",
    "- **objective [default=reg:linear]**\n",
    "    - This defines the loss function to be minimized. Mostly used values are:\n",
    "        - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "        - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "                - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "        - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "- **eval_metric [ default according to objective ]**\n",
    "    - The metric to be used for validation data.\n",
    "    - The default values are rmse for regression and error for classification.\n",
    "    - Typical values are:\n",
    "            - rmse – root mean square error\n",
    "            - mae – mean absolute error\n",
    "            - logloss – negative log-likelihood\n",
    "            - error – Binary classification error rate (0.5 threshold)\n",
    "            - merror – Multiclass classification error rate\n",
    "            - mlogloss – Multiclass logloss\n",
    "            - auc: Area under the curve\n",
    "- **seed [default=0]**\n",
    "    - The random number seed.\n",
    "    - Can be used for generating reproducible results and also for parameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.3, \n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 2, \n",
    "                           alpha = 1, \n",
    "                           n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.784753\n",
      "F1: 0.675676\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation using XGBoost\n",
    "In order to build more robust models, it is common to do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. XGBoost supports k-fold cross validation via the cv() method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters (check out this link) like:\n",
    "\n",
    "- **num_boost_round**: denotes the number of trees you build (analogous to n_estimators)\n",
    "- **metrics:** tells the evaluation metrics to be watched during CV\n",
    "- **as_pandas**: to return the results in a pandas DataFrame.\n",
    "- **early_stopping_rounds: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "- **seed**: for reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running your model, you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flatironschool/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/Users/flatironschool/anaconda3/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    }
   ],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\":\"binary:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 2, \n",
    "          'alpha': 1}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=5,\n",
    "                    num_boost_round=500,\n",
    "                    early_stopping_rounds=5,\n",
    "                    metrics=\"logloss\", \n",
    "                    as_pandas=True, \n",
    "                    seed=123)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.659772</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.660168</td>\n",
       "      <td>0.001478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.640294</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>0.640921</td>\n",
       "      <td>0.011085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.629094</td>\n",
       "      <td>0.006565</td>\n",
       "      <td>0.630053</td>\n",
       "      <td>0.011036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.616500</td>\n",
       "      <td>0.011333</td>\n",
       "      <td>0.617743</td>\n",
       "      <td>0.016020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.601325</td>\n",
       "      <td>0.011162</td>\n",
       "      <td>0.602671</td>\n",
       "      <td>0.015340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.590203</td>\n",
       "      <td>0.014328</td>\n",
       "      <td>0.592608</td>\n",
       "      <td>0.015515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.580331</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.583432</td>\n",
       "      <td>0.011188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.570725</td>\n",
       "      <td>0.015010</td>\n",
       "      <td>0.574913</td>\n",
       "      <td>0.009321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.564326</td>\n",
       "      <td>0.016661</td>\n",
       "      <td>0.569265</td>\n",
       "      <td>0.009131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.557697</td>\n",
       "      <td>0.015285</td>\n",
       "      <td>0.563021</td>\n",
       "      <td>0.007609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.552730</td>\n",
       "      <td>0.015043</td>\n",
       "      <td>0.558481</td>\n",
       "      <td>0.007250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.547068</td>\n",
       "      <td>0.015693</td>\n",
       "      <td>0.552817</td>\n",
       "      <td>0.008987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.542089</td>\n",
       "      <td>0.015239</td>\n",
       "      <td>0.548452</td>\n",
       "      <td>0.011089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.536571</td>\n",
       "      <td>0.017683</td>\n",
       "      <td>0.543687</td>\n",
       "      <td>0.011051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.531027</td>\n",
       "      <td>0.013537</td>\n",
       "      <td>0.538226</td>\n",
       "      <td>0.012720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.528377</td>\n",
       "      <td>0.012291</td>\n",
       "      <td>0.535746</td>\n",
       "      <td>0.012291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.523079</td>\n",
       "      <td>0.008863</td>\n",
       "      <td>0.531756</td>\n",
       "      <td>0.013968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.518823</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>0.527936</td>\n",
       "      <td>0.016220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.516713</td>\n",
       "      <td>0.006924</td>\n",
       "      <td>0.525689</td>\n",
       "      <td>0.017301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.512552</td>\n",
       "      <td>0.004859</td>\n",
       "      <td>0.521564</td>\n",
       "      <td>0.015942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.508500</td>\n",
       "      <td>0.005185</td>\n",
       "      <td>0.518616</td>\n",
       "      <td>0.013734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.503175</td>\n",
       "      <td>0.002684</td>\n",
       "      <td>0.513420</td>\n",
       "      <td>0.017039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.500755</td>\n",
       "      <td>0.003567</td>\n",
       "      <td>0.511212</td>\n",
       "      <td>0.017609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.498579</td>\n",
       "      <td>0.004905</td>\n",
       "      <td>0.509316</td>\n",
       "      <td>0.019612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.497073</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.508617</td>\n",
       "      <td>0.020337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.493866</td>\n",
       "      <td>0.006596</td>\n",
       "      <td>0.505923</td>\n",
       "      <td>0.018245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.492352</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.505034</td>\n",
       "      <td>0.018548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.488268</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.501230</td>\n",
       "      <td>0.021190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.485094</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>0.498670</td>\n",
       "      <td>0.023099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.483280</td>\n",
       "      <td>0.008005</td>\n",
       "      <td>0.497143</td>\n",
       "      <td>0.024599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.405556</td>\n",
       "      <td>0.010567</td>\n",
       "      <td>0.438868</td>\n",
       "      <td>0.033639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.405416</td>\n",
       "      <td>0.010647</td>\n",
       "      <td>0.438799</td>\n",
       "      <td>0.033762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.405035</td>\n",
       "      <td>0.010488</td>\n",
       "      <td>0.438441</td>\n",
       "      <td>0.034176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.404769</td>\n",
       "      <td>0.010535</td>\n",
       "      <td>0.438322</td>\n",
       "      <td>0.034408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.404579</td>\n",
       "      <td>0.010568</td>\n",
       "      <td>0.438402</td>\n",
       "      <td>0.034433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.404267</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.438157</td>\n",
       "      <td>0.034757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.403527</td>\n",
       "      <td>0.010805</td>\n",
       "      <td>0.437963</td>\n",
       "      <td>0.034813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.403150</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>0.437628</td>\n",
       "      <td>0.034788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.402852</td>\n",
       "      <td>0.010817</td>\n",
       "      <td>0.437338</td>\n",
       "      <td>0.035082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.402380</td>\n",
       "      <td>0.010784</td>\n",
       "      <td>0.437535</td>\n",
       "      <td>0.035311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.402135</td>\n",
       "      <td>0.010734</td>\n",
       "      <td>0.437321</td>\n",
       "      <td>0.035330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.401965</td>\n",
       "      <td>0.010690</td>\n",
       "      <td>0.437123</td>\n",
       "      <td>0.035649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.401602</td>\n",
       "      <td>0.010874</td>\n",
       "      <td>0.437043</td>\n",
       "      <td>0.036012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.401007</td>\n",
       "      <td>0.011147</td>\n",
       "      <td>0.436706</td>\n",
       "      <td>0.035821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.400863</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>0.436732</td>\n",
       "      <td>0.035924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.400192</td>\n",
       "      <td>0.011175</td>\n",
       "      <td>0.436602</td>\n",
       "      <td>0.036186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.399852</td>\n",
       "      <td>0.011498</td>\n",
       "      <td>0.436334</td>\n",
       "      <td>0.035925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.399639</td>\n",
       "      <td>0.011438</td>\n",
       "      <td>0.436299</td>\n",
       "      <td>0.035879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.399345</td>\n",
       "      <td>0.011369</td>\n",
       "      <td>0.436111</td>\n",
       "      <td>0.035887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.399158</td>\n",
       "      <td>0.011383</td>\n",
       "      <td>0.436208</td>\n",
       "      <td>0.036155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.398955</td>\n",
       "      <td>0.011392</td>\n",
       "      <td>0.436230</td>\n",
       "      <td>0.036228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.398512</td>\n",
       "      <td>0.011461</td>\n",
       "      <td>0.436171</td>\n",
       "      <td>0.036488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.398139</td>\n",
       "      <td>0.011486</td>\n",
       "      <td>0.436183</td>\n",
       "      <td>0.036983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.397733</td>\n",
       "      <td>0.011659</td>\n",
       "      <td>0.435879</td>\n",
       "      <td>0.037050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.397330</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>0.435761</td>\n",
       "      <td>0.037144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.396885</td>\n",
       "      <td>0.011926</td>\n",
       "      <td>0.435700</td>\n",
       "      <td>0.037383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.396632</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.435757</td>\n",
       "      <td>0.037410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.396027</td>\n",
       "      <td>0.011676</td>\n",
       "      <td>0.435443</td>\n",
       "      <td>0.037449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.395488</td>\n",
       "      <td>0.011790</td>\n",
       "      <td>0.435333</td>\n",
       "      <td>0.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.394918</td>\n",
       "      <td>0.011901</td>\n",
       "      <td>0.435028</td>\n",
       "      <td>0.037540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0              0.659772           0.000850           0.660168   \n",
       "1              0.640294           0.007082           0.640921   \n",
       "2              0.629094           0.006565           0.630053   \n",
       "3              0.616500           0.011333           0.617743   \n",
       "4              0.601325           0.011162           0.602671   \n",
       "5              0.590203           0.014328           0.592608   \n",
       "6              0.580331           0.011428           0.583432   \n",
       "7              0.570725           0.015010           0.574913   \n",
       "8              0.564326           0.016661           0.569265   \n",
       "9              0.557697           0.015285           0.563021   \n",
       "10             0.552730           0.015043           0.558481   \n",
       "11             0.547068           0.015693           0.552817   \n",
       "12             0.542089           0.015239           0.548452   \n",
       "13             0.536571           0.017683           0.543687   \n",
       "14             0.531027           0.013537           0.538226   \n",
       "15             0.528377           0.012291           0.535746   \n",
       "16             0.523079           0.008863           0.531756   \n",
       "17             0.518823           0.006483           0.527936   \n",
       "18             0.516713           0.006924           0.525689   \n",
       "19             0.512552           0.004859           0.521564   \n",
       "20             0.508500           0.005185           0.518616   \n",
       "21             0.503175           0.002684           0.513420   \n",
       "22             0.500755           0.003567           0.511212   \n",
       "23             0.498579           0.004905           0.509316   \n",
       "24             0.497073           0.005650           0.508617   \n",
       "25             0.493866           0.006596           0.505923   \n",
       "26             0.492352           0.006470           0.505034   \n",
       "27             0.488268           0.004673           0.501230   \n",
       "28             0.485094           0.007452           0.498670   \n",
       "29             0.483280           0.008005           0.497143   \n",
       "..                  ...                ...                ...   \n",
       "100            0.405556           0.010567           0.438868   \n",
       "101            0.405416           0.010647           0.438799   \n",
       "102            0.405035           0.010488           0.438441   \n",
       "103            0.404769           0.010535           0.438322   \n",
       "104            0.404579           0.010568           0.438402   \n",
       "105            0.404267           0.010597           0.438157   \n",
       "106            0.403527           0.010805           0.437963   \n",
       "107            0.403150           0.010858           0.437628   \n",
       "108            0.402852           0.010817           0.437338   \n",
       "109            0.402380           0.010784           0.437535   \n",
       "110            0.402135           0.010734           0.437321   \n",
       "111            0.401965           0.010690           0.437123   \n",
       "112            0.401602           0.010874           0.437043   \n",
       "113            0.401007           0.011147           0.436706   \n",
       "114            0.400863           0.011151           0.436732   \n",
       "115            0.400192           0.011175           0.436602   \n",
       "116            0.399852           0.011498           0.436334   \n",
       "117            0.399639           0.011438           0.436299   \n",
       "118            0.399345           0.011369           0.436111   \n",
       "119            0.399158           0.011383           0.436208   \n",
       "120            0.398955           0.011392           0.436230   \n",
       "121            0.398512           0.011461           0.436171   \n",
       "122            0.398139           0.011486           0.436183   \n",
       "123            0.397733           0.011659           0.435879   \n",
       "124            0.397330           0.011819           0.435761   \n",
       "125            0.396885           0.011926           0.435700   \n",
       "126            0.396632           0.011839           0.435757   \n",
       "127            0.396027           0.011676           0.435443   \n",
       "128            0.395488           0.011790           0.435333   \n",
       "129            0.394918           0.011901           0.435028   \n",
       "\n",
       "     test-logloss-std  \n",
       "0            0.001478  \n",
       "1            0.011085  \n",
       "2            0.011036  \n",
       "3            0.016020  \n",
       "4            0.015340  \n",
       "5            0.015515  \n",
       "6            0.011188  \n",
       "7            0.009321  \n",
       "8            0.009131  \n",
       "9            0.007609  \n",
       "10           0.007250  \n",
       "11           0.008987  \n",
       "12           0.011089  \n",
       "13           0.011051  \n",
       "14           0.012720  \n",
       "15           0.012291  \n",
       "16           0.013968  \n",
       "17           0.016220  \n",
       "18           0.017301  \n",
       "19           0.015942  \n",
       "20           0.013734  \n",
       "21           0.017039  \n",
       "22           0.017609  \n",
       "23           0.019612  \n",
       "24           0.020337  \n",
       "25           0.018245  \n",
       "26           0.018548  \n",
       "27           0.021190  \n",
       "28           0.023099  \n",
       "29           0.024599  \n",
       "..                ...  \n",
       "100          0.033639  \n",
       "101          0.033762  \n",
       "102          0.034176  \n",
       "103          0.034408  \n",
       "104          0.034433  \n",
       "105          0.034757  \n",
       "106          0.034813  \n",
       "107          0.034788  \n",
       "108          0.035082  \n",
       "109          0.035311  \n",
       "110          0.035330  \n",
       "111          0.035649  \n",
       "112          0.036012  \n",
       "113          0.035821  \n",
       "114          0.035924  \n",
       "115          0.036186  \n",
       "116          0.035925  \n",
       "117          0.035879  \n",
       "118          0.035887  \n",
       "119          0.036155  \n",
       "120          0.036228  \n",
       "121          0.036488  \n",
       "122          0.036983  \n",
       "123          0.037050  \n",
       "124          0.037144  \n",
       "125          0.037383  \n",
       "126          0.037410  \n",
       "127          0.037449  \n",
       "128          0.037500  \n",
       "129          0.037540  \n",
       "\n",
       "[130 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_clf)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "\n",
    "    return alg\n",
    "#     feat_imp = pd.Series(alg.get_booster().get_fscore())\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.concat([X_train, y_train], axis=1)\n",
    "target = 'Survived'\n",
    "IDcol = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  S  \\\n",
       "PassengerId                                                             \n",
       "740               3  24.0      0      0   7.8958        0     1  0  1   \n",
       "148               3   9.0      2      2  34.3750        1     0  0  1   \n",
       "876               3  15.0      0      0   7.2250        0     0  0  0   \n",
       "641               3  20.0      0      0   7.8542        0     1  0  1   \n",
       "885               3  25.0      0      0   7.0500        0     1  0  1   \n",
       "\n",
       "             Survived  \n",
       "PassengerId            \n",
       "740                 0  \n",
       "148                 0  \n",
       "876                 1  \n",
       "641                 0  \n",
       "885                 0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8919\n",
      "AUC Score (Train): 0.939281\n"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.6,\n",
    " colsample_bytree=0.3,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "alg = modelfit(xgb1, train, predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802691\n",
      "F1: 0.702703\n"
     ]
    }
   ],
   "source": [
    "preds = alg.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining XGBoost with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup our parameters for XGBoost to test, and you can alter all of these value in order to up the F1\n",
    "param_test1 = {\n",
    " 'max_depth':range(2,9,2),\n",
    " 'min_child_weight':range(1,6,2),\n",
    "    'colsample_bytree': [0.2,0.3,0.4,0.5]\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the Gridsearch model\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27),\n",
    "    param_grid = param_test1, \n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    iid=False, \n",
    "    cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=1, missing=None, n_estimators=140, n_jobs=1,\n",
       "       nthread=4, objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=27, silent=None,\n",
       "       subsample=0.8, verbosity=1),\n",
       "       fit_params=None, iid=False, n_jobs=-1,\n",
       "       param_grid={'max_depth': range(2, 9, 2), 'min_child_weight': range(1, 6, 2), 'colsample_bytree': [0.2, 0.3, 0.4, 0.5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.fit(train[predictors],train[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flatironschool/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/flatironschool/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/flatironschool/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/flatironschool/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/flatironschool/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/flatironschool/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/flatironschool/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.12579098, 0.08810053, 0.25472527, 0.172439  , 0.10025029,\n",
       "        0.11756964, 0.18536758, 0.20371819, 0.14200125, 0.11549478,\n",
       "        0.19766335, 0.17389984, 0.09996724, 0.09339881, 0.10013962,\n",
       "        0.10510149, 0.09131904, 0.12220163, 0.10981574, 0.13672061,\n",
       "        0.11979318, 0.11512098, 0.10610399, 0.09907937, 0.09007382,\n",
       "        0.08075328, 0.09756222, 0.11370683, 0.10404472, 0.15290689,\n",
       "        0.15115108, 0.1145472 , 0.11062312, 0.12317209, 0.11647539,\n",
       "        0.10736041, 0.08925915, 0.0895421 , 0.10107474, 0.13687491,\n",
       "        0.15838327, 0.18213677, 0.14872675, 0.12566619, 0.16317472,\n",
       "        0.20720963, 0.16064563, 0.11097856]),\n",
       " 'std_fit_time': array([0.04405197, 0.01163032, 0.0855755 , 0.03370253, 0.00976147,\n",
       "        0.04162567, 0.0386912 , 0.01828501, 0.05334776, 0.03583875,\n",
       "        0.00670261, 0.04456279, 0.01433616, 0.01199302, 0.04336636,\n",
       "        0.01434139, 0.0055368 , 0.0063633 , 0.00413634, 0.02741339,\n",
       "        0.01650559, 0.00605183, 0.00979002, 0.00857586, 0.00976694,\n",
       "        0.00401191, 0.01500508, 0.01314496, 0.00596713, 0.00974759,\n",
       "        0.03246638, 0.0069176 , 0.00206262, 0.00222806, 0.00348992,\n",
       "        0.00774913, 0.0134725 , 0.00826227, 0.01162444, 0.01392211,\n",
       "        0.02995737, 0.0135346 , 0.02802501, 0.01001731, 0.02919969,\n",
       "        0.02720201, 0.01032512, 0.01196406]),\n",
       " 'mean_score_time': array([0.00775795, 0.00853605, 0.05031352, 0.00778155, 0.00581326,\n",
       "        0.00771241, 0.01401548, 0.00968676, 0.00678425, 0.00785308,\n",
       "        0.0290278 , 0.02269015, 0.00508904, 0.00555587, 0.00942059,\n",
       "        0.00490532, 0.00633106, 0.00532374, 0.00607066, 0.00902839,\n",
       "        0.00533767, 0.00552773, 0.005092  , 0.0047698 , 0.00463204,\n",
       "        0.00472436, 0.0048852 , 0.00476074, 0.00609941, 0.00699162,\n",
       "        0.00531597, 0.00589485, 0.00496702, 0.00539961, 0.0062367 ,\n",
       "        0.00636883, 0.00435777, 0.00564213, 0.01087489, 0.00664444,\n",
       "        0.00861712, 0.01094546, 0.00587769, 0.00533543, 0.00975099,\n",
       "        0.00783463, 0.00608478, 0.0049253 ]),\n",
       " 'std_score_time': array([0.00382256, 0.00453815, 0.0637526 , 0.00313891, 0.00161517,\n",
       "        0.00238724, 0.00653487, 0.00370375, 0.00219167, 0.00525064,\n",
       "        0.04450204, 0.02294754, 0.00131581, 0.00101927, 0.00821694,\n",
       "        0.00031315, 0.00210444, 0.00059328, 0.00182023, 0.0021433 ,\n",
       "        0.00067954, 0.00064971, 0.00029121, 0.00017021, 0.00058294,\n",
       "        0.00100135, 0.00075033, 0.0001326 , 0.00219445, 0.00274204,\n",
       "        0.00025213, 0.00095244, 0.00019642, 0.00020293, 0.00191293,\n",
       "        0.00194139, 0.00016585, 0.00254361, 0.00893125, 0.0030318 ,\n",
       "        0.00406271, 0.00355739, 0.00146033, 0.00043447, 0.00726068,\n",
       "        0.00216283, 0.00094525, 0.00212721]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4,\n",
       "                    6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8,\n",
       "                    2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_child_weight': masked_array(data=[1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.2, 'max_depth': 2, 'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2, 'max_depth': 2, 'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2, 'max_depth': 2, 'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2, 'max_depth': 4, 'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2, 'max_depth': 4, 'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2, 'max_depth': 4, 'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2, 'max_depth': 6, 'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2, 'max_depth': 6, 'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2, 'max_depth': 6, 'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2, 'max_depth': 8, 'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2, 'max_depth': 8, 'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2, 'max_depth': 8, 'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3, 'max_depth': 2, 'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3, 'max_depth': 2, 'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3, 'max_depth': 2, 'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3, 'max_depth': 4, 'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3, 'max_depth': 4, 'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3, 'max_depth': 4, 'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3, 'max_depth': 6, 'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3, 'max_depth': 6, 'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3, 'max_depth': 6, 'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3, 'max_depth': 8, 'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3, 'max_depth': 8, 'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3, 'max_depth': 8, 'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4, 'max_depth': 2, 'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4, 'max_depth': 2, 'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4, 'max_depth': 2, 'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4, 'max_depth': 4, 'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4, 'max_depth': 4, 'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4, 'max_depth': 4, 'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4, 'max_depth': 6, 'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4, 'max_depth': 6, 'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4, 'max_depth': 6, 'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4, 'max_depth': 8, 'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4, 'max_depth': 8, 'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4, 'max_depth': 8, 'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5, 'max_depth': 2, 'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5, 'max_depth': 2, 'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5, 'max_depth': 2, 'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5, 'max_depth': 4, 'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5, 'max_depth': 4, 'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5, 'max_depth': 4, 'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5, 'max_depth': 6, 'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5, 'max_depth': 6, 'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5, 'max_depth': 6, 'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5, 'max_depth': 8, 'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5, 'max_depth': 8, 'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5, 'max_depth': 8, 'min_child_weight': 5}],\n",
       " 'split0_test_score': array([0.89587701, 0.89389704, 0.89447939, 0.89983694, 0.89121826,\n",
       "        0.89098532, 0.89657582, 0.88842301, 0.89028651, 0.89121826,\n",
       "        0.88655952, 0.88865595, 0.91101794, 0.90740741, 0.89983694,\n",
       "        0.90845563, 0.91194969, 0.9089215 , 0.91660843, 0.91101794,\n",
       "        0.90868856, 0.90798975, 0.90822269, 0.90822269, 0.91649196,\n",
       "        0.91043559, 0.90752388, 0.92173305, 0.91893781, 0.91590962,\n",
       "        0.91358025, 0.91288143, 0.91660843, 0.91218262, 0.91870487,\n",
       "        0.91311437, 0.91497787, 0.91055206, 0.91241556, 0.91917074,\n",
       "        0.91963662, 0.91684137, 0.91940368, 0.92382949, 0.91963662,\n",
       "        0.91358025, 0.92872117, 0.91823899]),\n",
       " 'split1_test_score': array([0.81038901, 0.80433263, 0.79920801, 0.80654554, 0.80142092,\n",
       "        0.78895877, 0.80421617, 0.79676217, 0.78942464, 0.80607966,\n",
       "        0.80095504, 0.78919171, 0.82518053, 0.82587934, 0.82180294,\n",
       "        0.83473096, 0.83170277, 0.81924062, 0.82983927, 0.8331004 ,\n",
       "        0.82692756, 0.82494759, 0.83426508, 0.82552993, 0.82669462,\n",
       "        0.8331004 , 0.82716049, 0.83193571, 0.83542977, 0.82809224,\n",
       "        0.83496389, 0.84195201, 0.8315863 , 0.83845795, 0.84591195,\n",
       "        0.83554624, 0.82995574, 0.8315863 , 0.82646168, 0.82960634,\n",
       "        0.8293734 , 0.83624505, 0.83193571, 0.83379921, 0.83205218,\n",
       "        0.84195201, 0.84078733, 0.8334498 ]),\n",
       " 'split2_test_score': array([0.82541346, 0.83228512, 0.83007221, 0.82401584, 0.82273468,\n",
       "        0.82483112, 0.81551363, 0.82063825, 0.82180294, 0.80642907,\n",
       "        0.82273468, 0.82320056, 0.83473096, 0.8331004 , 0.83624505,\n",
       "        0.82168647, 0.825297  , 0.82692756, 0.82541346, 0.82599581,\n",
       "        0.8315863 , 0.82622874, 0.8234335 , 0.83181924, 0.83834149,\n",
       "        0.84393198, 0.83869089, 0.8297228 , 0.82995574, 0.83461449,\n",
       "        0.84183555, 0.83694386, 0.83228512, 0.84067086, 0.8353133 ,\n",
       "        0.83112043, 0.84323317, 0.83892383, 0.83508036, 0.83973911,\n",
       "        0.83787561, 0.83403215, 0.84253436, 0.84113673, 0.83985558,\n",
       "        0.84649429, 0.83577918, 0.8393897 ]),\n",
       " 'split3_test_score': array([0.92163462, 0.91346154, 0.90721154, 0.91274038, 0.89591346,\n",
       "        0.89867788, 0.91346154, 0.89302885, 0.8984375 , 0.90625   ,\n",
       "        0.89615385, 0.89627404, 0.91117788, 0.90949519, 0.91057692,\n",
       "        0.91081731, 0.90432692, 0.90492788, 0.90576923, 0.896875  ,\n",
       "        0.91021635, 0.90336538, 0.89735577, 0.91045673, 0.91682692,\n",
       "        0.91105769, 0.90985577, 0.90793269, 0.90384615, 0.91382212,\n",
       "        0.90384615, 0.90793269, 0.91262019, 0.90360577, 0.90961538,\n",
       "        0.90877404, 0.91322115, 0.90925481, 0.90877404, 0.89831731,\n",
       "        0.89639423, 0.90949519, 0.90288462, 0.90504808, 0.91189904,\n",
       "        0.89879808, 0.90697115, 0.91454327]),\n",
       " 'split4_test_score': array([0.87271635, 0.87331731, 0.87764423, 0.88088942, 0.87475962,\n",
       "        0.88100962, 0.87259615, 0.87956731, 0.88016827, 0.87259615,\n",
       "        0.87908654, 0.87896635, 0.88557692, 0.87944712, 0.88353365,\n",
       "        0.89591346, 0.88605769, 0.8890625 , 0.88389423, 0.88293269,\n",
       "        0.89338942, 0.88581731, 0.88918269, 0.89242788, 0.88954327,\n",
       "        0.87992788, 0.88762019, 0.88581731, 0.88293269, 0.89278846,\n",
       "        0.88990385, 0.88677885, 0.89447115, 0.89471154, 0.88870192,\n",
       "        0.89423077, 0.88858173, 0.88185096, 0.89170673, 0.88629808,\n",
       "        0.88762019, 0.89242788, 0.89615385, 0.88677885, 0.89663462,\n",
       "        0.89399038, 0.89375   , 0.89507212]),\n",
       " 'mean_test_score': array([0.86520609, 0.86345873, 0.86172308, 0.86480563, 0.85720939,\n",
       "        0.85689254, 0.86047266, 0.85568392, 0.85602397, 0.85651463,\n",
       "        0.85709793, 0.85525772, 0.87353685, 0.87106589, 0.8703991 ,\n",
       "        0.87432076, 0.87186681, 0.86981601, 0.87230493, 0.86998437,\n",
       "        0.87416164, 0.86966976, 0.87049195, 0.8736913 , 0.87757965,\n",
       "        0.87569071, 0.87417024, 0.87542831, 0.87422043, 0.87704539,\n",
       "        0.87682594, 0.87729777, 0.87751424, 0.87792575, 0.87964949,\n",
       "        0.87655717, 0.87799393, 0.87443359, 0.87488767, 0.87462631,\n",
       "        0.87418001, 0.87780833, 0.87858244, 0.87811847, 0.88001561,\n",
       "        0.878963  , 0.88120177, 0.88013878]),\n",
       " 'std_test_score': array([0.04187996, 0.03997913, 0.04075677, 0.04205156, 0.03811424,\n",
       "        0.0427385 , 0.04346112, 0.0393367 , 0.0428073 , 0.04240003,\n",
       "        0.0379729 , 0.0418384 , 0.03690765, 0.03563977, 0.03516091,\n",
       "        0.03821296, 0.03645154, 0.0388067 , 0.03799767, 0.03426351,\n",
       "        0.03716309, 0.03674712, 0.03470303, 0.03733085, 0.03827941,\n",
       "        0.03255314, 0.03474341, 0.03818166, 0.03582543, 0.03822981,\n",
       "        0.03233809, 0.03216317, 0.03795621, 0.03181318, 0.03349371,\n",
       "        0.0358695 , 0.03531812, 0.03367168, 0.03679458, 0.03442561,\n",
       "        0.03483118, 0.03573577, 0.03475947, 0.03527478, 0.03681274,\n",
       "        0.02912606, 0.03681345, 0.03660232]),\n",
       " 'rank_test_score': array([37, 39, 40, 38, 42, 44, 41, 47, 46, 45, 43, 48, 28, 31, 33, 22, 30,\n",
       "        35, 29, 34, 26, 36, 32, 27, 11, 17, 25, 18, 23, 14, 15, 13, 12,  9,\n",
       "         4, 16,  8, 21, 19, 20, 24, 10,  6,  7,  3,  5,  1,  2],\n",
       "       dtype=int32),\n",
       " 'split0_train_score': array([0.89031352, 0.88831707, 0.88361432, 0.91329488, 0.90786749,\n",
       "        0.89679828, 0.929126  , 0.91495859, 0.90199645, 0.93907867,\n",
       "        0.91696983, 0.90326087, 0.90671399, 0.90208518, 0.89611062,\n",
       "        0.9399512 , 0.92920734, 0.91983881, 0.96188258, 0.94179976,\n",
       "        0.92454156, 0.96840432, 0.94478705, 0.92436409, 0.91538007,\n",
       "        0.90901361, 0.90187814, 0.95754215, 0.9386572 , 0.92700385,\n",
       "        0.97775067, 0.95340875, 0.93410234, 0.98533718, 0.95735729,\n",
       "        0.93433895, 0.91994232, 0.91311003, 0.90672138, 0.96346495,\n",
       "        0.94719757, 0.93368086, 0.98252736, 0.96139456, 0.94094203,\n",
       "        0.99030612, 0.96523957, 0.94240609]),\n",
       " 'split1_train_score': array([0.90770482, 0.90602632, 0.89991127, 0.92661934, 0.91805679,\n",
       "        0.90878438, 0.93938184, 0.92620526, 0.91390121, 0.9476782 ,\n",
       "        0.9300281 , 0.91608252, 0.92661195, 0.92380213, 0.91629695,\n",
       "        0.95419255, 0.94315291, 0.93418367, 0.96770186, 0.95244011,\n",
       "        0.93902692, 0.97627921, 0.95402248, 0.93969979, 0.93284531,\n",
       "        0.92907424, 0.92312186, 0.96605294, 0.95258799, 0.9425244 ,\n",
       "        0.97966578, 0.96018929, 0.94655427, 0.98532978, 0.96351671,\n",
       "        0.94747856, 0.93896776, 0.93192842, 0.92659716, 0.97307749,\n",
       "        0.95981958, 0.94620674, 0.98609879, 0.96666667, 0.95058415,\n",
       "        0.99167406, 0.96910677, 0.9523144 ]),\n",
       " 'split2_train_score': array([0.90499852, 0.9011535 , 0.8984546 , 0.92846791, 0.91876664,\n",
       "        0.90928719, 0.94168885, 0.92439367, 0.91223011, 0.95147885,\n",
       "        0.92813517, 0.91265898, 0.92495563, 0.92118456, 0.91626738,\n",
       "        0.95477669, 0.9440698 , 0.93413191, 0.96864833, 0.95199645,\n",
       "        0.93715617, 0.97453416, 0.95448092, 0.93820615, 0.931189  ,\n",
       "        0.92572464, 0.92136202, 0.9649438 , 0.95190033, 0.94089027,\n",
       "        0.98044957, 0.96109879, 0.94526028, 0.98562555, 0.96182343,\n",
       "        0.94612541, 0.93467169, 0.92891156, 0.92400917, 0.97295179,\n",
       "        0.95857734, 0.94685744, 0.98787341, 0.96759095, 0.95146406,\n",
       "        0.99114167, 0.96930642, 0.95113872]),\n",
       " 'split3_train_score': array([0.88954998, 0.89059909, 0.8851628 , 0.9123883 , 0.90805981,\n",
       "        0.8963802 , 0.9265036 , 0.91321732, 0.90210996, 0.93443429,\n",
       "        0.91676815, 0.90316641, 0.91122181, 0.90712808, 0.90004842,\n",
       "        0.94526286, 0.9333485 , 0.91989347, 0.96207064, 0.94460259,\n",
       "        0.92921808, 0.97245903, 0.94914384, 0.93024518, 0.91597582,\n",
       "        0.91454521, 0.90656317, 0.95806494, 0.94413305, 0.92973897,\n",
       "        0.97515883, 0.95532112, 0.93703139, 0.98286209, 0.95931214,\n",
       "        0.93815386, 0.9223952 , 0.92105997, 0.91213153, 0.9687688 ,\n",
       "        0.95097061, 0.93795578, 0.98432938, 0.96445498, 0.94524086,\n",
       "        0.98956759, 0.96499787, 0.94525553]),\n",
       " 'split4_train_score': array([0.8940032 , 0.89166288, 0.88896307, 0.9152275 , 0.90506654,\n",
       "        0.89928543, 0.92841841, 0.91539624, 0.90364327, 0.93794844,\n",
       "        0.91800801, 0.90480243, 0.91454521, 0.90910892, 0.90490514,\n",
       "        0.946378  , 0.93571816, 0.92521239, 0.96535736, 0.94604786,\n",
       "        0.93358326, 0.97310463, 0.95068449, 0.93362728, 0.92281338,\n",
       "        0.91811072, 0.91272578, 0.95978167, 0.94603319, 0.93568148,\n",
       "        0.97731575, 0.95888662, 0.94090502, 0.98363975, 0.96101419,\n",
       "        0.94163867, 0.93001775, 0.92417795, 0.91570437, 0.96873212,\n",
       "        0.95389785, 0.94116913, 0.98476956, 0.9634132 , 0.94722903,\n",
       "        0.98980236, 0.9673602 , 0.94791132]),\n",
       " 'mean_train_score': array([0.89731401, 0.89555177, 0.89122121, 0.91919959, 0.91156345,\n",
       "        0.9021071 , 0.93302374, 0.91883422, 0.9067762 , 0.94212369,\n",
       "        0.92198185, 0.90799424, 0.91680972, 0.91266177, 0.9067257 ,\n",
       "        0.94811226, 0.93709934, 0.92665205, 0.96513215, 0.94737735,\n",
       "        0.9327052 , 0.97295627, 0.95062375, 0.9332285 , 0.92364071,\n",
       "        0.91929368, 0.9131302 , 0.9612771 , 0.94666235, 0.93516779,\n",
       "        0.97806812, 0.95778091, 0.94077066, 0.98455887, 0.96060475,\n",
       "        0.94154709, 0.92919895, 0.92383759, 0.91703272, 0.96939903,\n",
       "        0.95409259, 0.94117399, 0.9851197 , 0.96470407, 0.94709202,\n",
       "        0.99049836, 0.96720217, 0.94780521]),\n",
       " 'std_train_score': array([0.00757983, 0.00682773, 0.00674548, 0.00689915, 0.00569552,\n",
       "        0.005746  , 0.00623575, 0.00535966, 0.00519511, 0.00639067,\n",
       "        0.00584294, 0.0053494 , 0.00775521, 0.00838873, 0.00828532,\n",
       "        0.00564117, 0.00571827, 0.00643187, 0.00279107, 0.00418435,\n",
       "        0.00527923, 0.00262759, 0.00354091, 0.00555503, 0.00733995,\n",
       "        0.00730265, 0.00821577, 0.00354281, 0.00516576, 0.00605396,\n",
       "        0.00186304, 0.0029393 , 0.00473442, 0.00110107, 0.0021142 ,\n",
       "        0.00489128, 0.00718369, 0.00654398, 0.00737915, 0.00352728,\n",
       "        0.00469547, 0.00498265, 0.00178936, 0.00223012, 0.00380993,\n",
       "        0.00079798, 0.00183254, 0.00366354])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5, 'max_depth': 8, 'min_child_weight': 3}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8812017662921751"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.780269\n",
      "F1: 0.666667\n"
     ]
    }
   ],
   "source": [
    "preds = gsearch1.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a253247f0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot feature importance\n",
    "plot_importance(alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fare': 231,\n",
       " 'Parch': 91,\n",
       " 'male': 42,\n",
       " 'Pclass': 81,\n",
       " 'SibSp': 86,\n",
       " 'S': 43,\n",
       " 'Q': 37,\n",
       " 'youngin': 27,\n",
       " 'Age': 150}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg.get_booster().get_fscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle list object\n",
    " \n",
    "model_pickle_path = 'xg_boost_model.pkl'\n",
    "\n",
    "# Create an variable to pickle and open it in write mode\n",
    "model_pickle = open(model_pickle_path, 'wb')\n",
    "pickle.dump(gsearch1.best_estimator_, model_pickle)\n",
    "model_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XGboost model ::  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=3, missing=nan, n_estimators=140, n_jobs=1,\n",
      "       nthread=4, objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "       reg_lambda=1, scale_pos_weight=1, seed=27, silent=None,\n",
      "       subsample=0.8, verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved XGboost model pickle\n",
    "xgboost_model_pkl = open(model_pickle_path, 'rb')\n",
    "xgboost_model = pickle.load(xgboost_model_pkl)\n",
    "print(\"Loaded XGboost model :: \", xgboost_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(9,12)\n",
    "    for min_child_weight in range(5,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 5), (9, 6), (9, 7), (10, 5), (10, 6), (10, 7), (11, 5), (11, 6), (11, 7)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
